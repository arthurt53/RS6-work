{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# 用户行为，使用format1进行加载\n",
    "# 加载全量样本\n",
    "\n",
    "user_log = pd.read_csv('./data_format1/user_log_format1.csv', dtype={'time_stamp':'str'})\n",
    "user_log_new = pd.read_csv('./data_format1/user_log_format1_new.csv')\n",
    "user_log['time_stamp_new'] = user_log_new['time_stamp_new']\n",
    "\n",
    "user_info = pd.read_csv('./data_format1/user_info_format1.csv')\n",
    "train_data1 = pd.read_csv('./data_format1/train_format1.csv')\n",
    "submission = pd.read_csv('./data_format1/test_format1.csv')\n",
    "\n",
    "\"\"\"\n",
    "# 加载小样本\n",
    "user_log = pd.read_csv('./data_format1_small/sample_user_log.csv', dtype={'time_stamp':'str'})\n",
    "user_info = pd.read_csv('./data_format1_small/sample_user_info.csv')\n",
    "train_data1 = pd.read_csv('./data_format1_small/train.csv')\n",
    "submission = pd.read_csv('./data_format1_small/test.csv')\n",
    "\"\"\"\n",
    "train_data = pd.read_csv('./data_format2/train_format2.csv')\n",
    "\n",
    "train_data1['origin'] = 'train'\n",
    "submission['origin'] = 'test'\n",
    "matrix = pd.concat([train_data1, submission], ignore_index=True, sort=False)\n",
    "#print(matrix)\n",
    "\n",
    "# 使用merchant_id（原列名seller_id）\n",
    "user_log.rename(columns={'seller_id':'merchant_id'}, inplace=True)\n",
    "# 格式化\n",
    "user_log['user_id'] = user_log['user_id'].astype('int32')\n",
    "user_log['merchant_id'] = user_log['merchant_id'].astype('int32')\n",
    "user_log['item_id'] = user_log['item_id'].astype('int32')\n",
    "user_log['cat_id'] = user_log['cat_id'].astype('int32')\n",
    "user_log['brand_id'].fillna(0, inplace=True)\n",
    "user_log['brand_id'] = user_log['brand_id'].astype('int32')\n",
    "user_log['time_stamp'] = pd.to_datetime(user_log['time_stamp'], format='%H%M')\n",
    "user_log['time_stamp_new'] = user_log['time_stamp_new'].astype('float32')\n",
    "\n",
    "# 对离散特征做LabelEncoder\n",
    "lbe_merchant_id=LabelEncoder()\n",
    "lbe_merchant_id.fit(np.r_[0,user_log['merchant_id'].values])\n",
    "user_log['merchant_id']=lbe_merchant_id.transform(user_log['merchant_id'])\n",
    "matrix['merchant_id']=lbe_merchant_id.transform(matrix['merchant_id'])\n",
    "\n",
    "lbe_user_id=LabelEncoder()\n",
    "user_log['user_id']=lbe_user_id.fit_transform(user_log['user_id'])\n",
    "user_info['user_id']=lbe_user_id.transform(user_info['user_id'])\n",
    "matrix['user_id']=lbe_user_id.transform(matrix['user_id'])\n",
    "\n",
    "lbe_item_id=LabelEncoder()\n",
    "user_log['item_id']=lbe_item_id.fit_transform(user_log['item_id'])\n",
    "lbe_cat_id=LabelEncoder()\n",
    "user_log['cat_id']=lbe_cat_id.fit_transform(user_log['cat_id'])\n",
    "lbe_brand_id=LabelEncoder()\n",
    "user_log['brand_id']=lbe_brand_id.fit_transform(user_log['brand_id'])\n",
    "\n",
    "user_log['merchant_id'].max(),user_log['user_id'].max()\n",
    "matrix = matrix.merge(user_info, on='user_id', how='left')\n",
    "\n",
    "# 1 for <18; 2 for [18,24]; 3 for [25,29]; 4 for [30,34]; 5 for [35,39]; 6 for [40,49]; 7 and 8 for >= 50; 0 and NULL for unknown\n",
    "matrix['age_range'].fillna(0, inplace=True)\n",
    "# 0:female, 1:male, 2:unknown\n",
    "matrix['gender'].fillna(2, inplace=True)\n",
    "matrix['age_range'] = matrix['age_range'].astype('int8')\n",
    "matrix['gender'] = matrix['gender'].astype('int8')\n",
    "matrix['label'] = matrix['label'].astype('str')\n",
    "matrix['user_id'] = matrix['user_id'].astype('int32')\n",
    "matrix['merchant_id'] = matrix['merchant_id'].astype('int32')\n",
    "del user_info, train_data1\n",
    "gc.collect()\n",
    "#print(matrix)\n",
    "\n",
    "# User特征处理\n",
    "groups = user_log.groupby(['user_id'])\n",
    "# 用户交互行为数量 u1\n",
    "temp = groups.size().reset_index().rename(columns={0:'u1'})\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "# 使用agg 基于列的聚合操作，统计唯一值的个数 item_id, cat_id, merchant_id, brand_id\n",
    "#temp = groups['item_id', 'cat_id', 'merchant_id', 'brand_id'].nunique().reset_index().rename(columns={'item_id':'u2', 'cat_id':'u3', 'merchant_id':'u4', 'brand_id':'u5'})\n",
    "temp = groups['item_id'].agg([('u2', 'nunique')]).reset_index()\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "temp = groups['cat_id'].agg([('u3', 'nunique')]).reset_index()\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "temp = groups['merchant_id'].agg([('u4', 'nunique')]).reset_index()\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "temp = groups['brand_id'].agg([('u5', 'nunique')]).reset_index()\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "\n",
    "# 时间间隔特征 u6 按照小时\n",
    "temp = groups['time_stamp'].agg([('F_time', 'min'), ('L_time', 'max')]).reset_index()\n",
    "temp['u6'] = (temp['L_time'] - temp['F_time']).dt.seconds/3600\n",
    "matrix = matrix.merge(temp[['user_id', 'u6']], on='user_id', how='left')\n",
    "# 统计action_type为0，1，2，3的个数（原始操作，没有补0）\n",
    "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'u7', 1:'u8', 2:'u9', 3:'u10'})\n",
    "matrix = matrix.merge(temp, on='user_id', how='left')\n",
    "#print(matrix)\n",
    "\n",
    "# 商家特征处理\n",
    "groups = user_log.groupby(['merchant_id'])\n",
    "# 商家被交互行为数量 m1\n",
    "temp = groups.size().reset_index().rename(columns={0:'m1'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "# 统计商家被交互的user_id, item_id, cat_id, brand_id 唯一值\n",
    "temp = groups['user_id', 'item_id', 'cat_id', 'brand_id'].nunique().reset_index().rename(columns={'user_id':'m2', 'item_id':'m3', 'cat_id':'m4', 'brand_id':'m5'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "# 统计商家被交互的action_type 唯一值\n",
    "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'m6', 1:'m7', 2:'m8', 3:'m9'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "# 按照merchant_id 统计随机负采样的个数\n",
    "temp = train_data[train_data['label']==-1].groupby(['merchant_id']).size().reset_index().rename(columns={0:'m10'})\n",
    "matrix = matrix.merge(temp, on='merchant_id', how='left')\n",
    "#print(matrix)\n",
    "\n",
    "# 按照user_id, merchant_id分组\n",
    "groups = user_log.groupby(['user_id', 'merchant_id'])\n",
    "temp = groups.size().reset_index().rename(columns={0:'um1'}) #统计行为个数\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
    "temp = groups['item_id', 'cat_id', 'brand_id'].nunique().reset_index().rename(columns={'item_id':'um2', 'cat_id':'um3', 'brand_id':'um4'}) #统计item_id, cat_id, brand_id唯一个数\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
    "temp = groups['action_type'].value_counts().unstack().reset_index().rename(columns={0:'um5', 1:'um6', 2:'um7', 3:'um8'})#统计不同action_type唯一个数\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left')\n",
    "temp = groups['time_stamp'].agg([('first', 'min'), ('last', 'max')]).reset_index()\n",
    "temp['um9'] = (temp['last'] - temp['first']).dt.seconds/3600\n",
    "temp.drop(['first', 'last'], axis=1, inplace=True)\n",
    "#print(temp)\n",
    "#print('-'*100)\n",
    "matrix = matrix.merge(temp, on=['user_id', 'merchant_id'], how='left') #统计时间间隔\n",
    "#print(matrix)\n",
    "\n",
    "#用户购买点击比\n",
    "matrix['r1'] = matrix['u9']/matrix['u7'] \n",
    "#商家购买点击比\n",
    "matrix['r2'] = matrix['m8']/matrix['m6'] \n",
    "#不同用户不同商家购买点击比\n",
    "matrix['r3'] = matrix['um7']/matrix['um5']\n",
    "matrix.fillna(0, inplace=True)\n",
    "# # 修改age_range字段名称为 age_0, age_1, age_2... age_8\n",
    "temp = pd.get_dummies(matrix['age_range'], prefix='age')\n",
    "matrix = pd.concat([matrix, temp], axis=1)\n",
    "temp = pd.get_dummies(matrix['gender'], prefix='g')\n",
    "matrix = pd.concat([matrix, temp], axis=1)\n",
    "matrix.drop(['age_range', 'gender'], axis=1, inplace=True)\n",
    "#print(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbe_action_type={0:1,1:2,2:3,3:4}\n",
    "user_log['action_type']=user_log['action_type'].map(lbe_action_type)\n",
    "# 用户行为sequence\n",
    "user_log_1 = pd.DataFrame(user_log.sort_values('time_stamp_new', ascending=True).groupby('user_id')['merchant_id','action_type','time_stamp_new'].agg(lambda x:list(x)))\n",
    "split_pot = list()\n",
    "for session in user_log_1['time_stamp_new']:\n",
    "    temp_list = list()\n",
    "    for session_num in range(1,len(session)):\n",
    "        time_num = (session[session_num] - session[session_num-1])\n",
    "        if time_num >= 0.5 :\n",
    "            temp_list.append(session_num)\n",
    "    split_pot.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "for count_1 in split_pot:\n",
    "    temp = len(count_1)\n",
    "    if temp > max_len:\n",
    "        max_len = temp\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 424170/424170 [8:48:56<00:00, 13.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "n = len(user_log_1['merchant_id'])\n",
    "merchant_id_list_all = pd.DataFrame(columns = [range(3)])\n",
    "\n",
    "for count_num in tqdm(range(n)):\n",
    "    merchant_id_temp = user_log_1['merchant_id'][count_num]\n",
    "    merchant_id_list_one = []\n",
    "    split_pot_n = split_pot[count_num]\n",
    "    if split_pot_n == []:\n",
    "        merchant_id_list_one.append(merchant_id_temp)\n",
    "    else:\n",
    "        if len(split_pot_n) > 2:\n",
    "            for count_2 in range(3):\n",
    "                if count_2 == 0:\n",
    "                    merchant_id_list_one.append(merchant_id_temp[0:split_pot_n[count_2]])\n",
    "                else:\n",
    "                    merchant_id_list_one.append(merchant_id_temp[split_pot_n[count_2-1]:split_pot_n[count_2]])\n",
    "        else:\n",
    "            for count_2 in range(len(split_pot_n)):\n",
    "                if count_2 == 0:\n",
    "                    merchant_id_list_one.append(merchant_id_temp[0:split_pot_n[count_2]])\n",
    "                else:\n",
    "                    merchant_id_list_one.append(merchant_id_temp[split_pot_n[count_2-1]:split_pot_n[count_2]])\n",
    "                if count_2 == len(split_pot_n)-1:\n",
    "                    merchant_id_list_one.append(merchant_id_temp[split_pot_n[count_2]:])                    \n",
    "    while len(merchant_id_list_one) < 3:\n",
    "        merchant_id_list_one.append([0])    \n",
    "    merchant_id_list_one_temp = pd.Series(merchant_id_list_one)\n",
    "    merchant_id_list_all = merchant_id_list_all.append(merchant_id_list_one_temp,ignore_index=True)\n",
    "\n",
    "#print(merchant_id_list_all)\n",
    "merchant_id_list_all.to_csv('merchant_id_list_all.csv',index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 424170/424170 [8:50:32<00:00, 13.32it/s]\n"
     ]
    }
   ],
   "source": [
    "n = len(user_log_1['action_type'])\n",
    "action_type_list_all = pd.DataFrame(columns = [range(3)])\n",
    "\n",
    "for count_num1 in tqdm(range(n)):\n",
    "    action_type_temp = user_log_1['action_type'][count_num1]\n",
    "    action_type_list_one = []\n",
    "    split_pot_n = split_pot[count_num1]\n",
    "    if split_pot_n == []:\n",
    "        action_type_list_one.append(action_type_temp)\n",
    "    else:\n",
    "        if len(split_pot_n) > 2:\n",
    "            for count_3 in range(3):\n",
    "                if count_3 == 0:\n",
    "                    action_type_list_one.append(action_type_temp[0:split_pot_n[count_3]])\n",
    "                else:\n",
    "                    action_type_list_one.append(action_type_temp[split_pot_n[count_3-1]:split_pot_n[count_3]])\n",
    "        else:\n",
    "            for count_4 in range(len(split_pot_n)):\n",
    "                if count_4 == 0:\n",
    "                    action_type_list_one.append(action_type_temp[0:split_pot_n[count_4]])\n",
    "                else:\n",
    "                    action_type_list_one.append(action_type_temp[split_pot_n[count_4-1]:split_pot_n[count_4]])\n",
    "                if count_4 == len(split_pot_n)-1:\n",
    "                    action_type_list_one.append(action_type_temp[split_pot_n[count_4]:])                    \n",
    "    while len(action_type_list_one) < 3:\n",
    "        action_type_list_one.append([0])    \n",
    "    action_type_list_one_temp = pd.Series(action_type_list_one)\n",
    "    action_type_list_all = action_type_list_all.append(action_type_list_one_temp,ignore_index=True)\n",
    "    #action_type_list_all = pd.concat([action_type_list_all,action_type_list_one_temp],axis = 0 ,ignore_index=True)\n",
    "\n",
    "#print(merchant_id_list_all)\n",
    "action_type_list_all.to_csv('action_type_list_all.csv',index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(action_type_list_all)\n",
    "for count_2 in range(3):\n",
    "    user_log_1['sess_' + str(count_2) + '_merchant_id'] = merchant_id_list_all[count_2]\n",
    "    user_log_1['sess_' + str(count_2) + '_action_type'] = action_type_list_all[count_2]\n",
    "#print(user_log_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = matrix.merge(user_log_1, on='user_id', how='left')\n",
    "#print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.to_csv('matrix.csv',index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "matrix = pd.read_csv('matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 522341/522341 [00:08<00:00, 61608.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 522341/522341 [00:08<00:00, 61784.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 522341/522341 [00:09<00:00, 54698.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 522341/522341 [00:09<00:00, 57142.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 522341/522341 [00:10<00:00, 48059.78it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 522341/522341 [00:08<00:00, 60760.89it/s]\n"
     ]
    }
   ],
   "source": [
    "for num in range(3):\n",
    "    temp_1 = matrix['sess_'+ str(num) + '_merchant_id']\n",
    "    temp_2 = matrix['sess_'+ str(num) + '_action_type']\n",
    "    temp_3 = 'sess_'+ str(num) + '_merchant_id'\n",
    "    temp_4 = 'sess_'+ str(num) + '_action_type'\n",
    "    for num_1 in tqdm(range(len(temp_1))):\n",
    "        temp = temp_1[num_1]\n",
    "        #print(temp)\n",
    "        merchant_id_list = temp[1:-1].strip().split(', ')\n",
    "        #print(merchant_id_list)\n",
    "        matrix.at[num_1,temp_3] = merchant_id_list\n",
    "\n",
    "    for num_2 in tqdm(range(len(temp_2))):\n",
    "        temp = temp_2[num_2]\n",
    "        action_type_list = temp[1:-1].strip().split(', ')\n",
    "        matrix.at[num_2, temp_4] = action_type_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [04:42<00:00, 94.21s/it]\n"
     ]
    }
   ],
   "source": [
    "M=500\n",
    "\n",
    "for num in tqdm(range(3)):\n",
    "    temp_1 = matrix['sess_'+ str(num) + '_merchant_id']\n",
    "    temp_2 = matrix['sess_'+ str(num) + '_action_type']\n",
    "    temp_3 = 'sess_'+ str(num) + '_merchant_id'\n",
    "    temp_4 = 'sess_'+ str(num) + '_action_type'\n",
    "    for merchant_id_num1 in range(len(temp_1)):\n",
    "        temp_5 = temp_1[merchant_id_num1]\n",
    "        #print(temp_5)\n",
    "        if len(temp_5)>M:\n",
    "            matrix.at[merchant_id_num1,temp_3] = temp_5[:M]\n",
    "        else:\n",
    "            while len(temp_5)<M:\n",
    "                temp_5.append('0')\n",
    "            matrix.at[merchant_id_num1,temp_3] = temp_5\n",
    "    for action_type_num1 in range(len(temp_1)):\n",
    "        temp_6 = temp_2[action_type_num1]\n",
    "        if len(temp_6)>M:\n",
    "            matrix.at[action_type_num1,temp_4] = temp_6[:M]\n",
    "        else:\n",
    "            while len(temp_6)<M:\n",
    "                temp_6.append('0')\n",
    "            matrix.at[action_type_num1,temp_4] = temp_6            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = matrix.drop('merchant_id_y',axis = 1)\n",
    "matrix = matrix.drop('action_type',axis = 1)\n",
    "matrix = matrix.drop('time_stamp_new',axis = 1)\n",
    "matrix = matrix.rename(columns={'merchant_id_x':'merchant_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 522341/522341 [48:44<00:00, 178.60it/s]\n"
     ]
    }
   ],
   "source": [
    "for num in tqdm(range(len(matrix))):\n",
    "    for num_1 in range(3):\n",
    "        temp_1 = matrix['sess_'+ str(num_1) + '_merchant_id']\n",
    "        temp_2 = temp_1[num]\n",
    "        temp_3 = temp_2[0]\n",
    "        if temp_3 == '0':\n",
    "            matrix.loc[num,'sess_length'] = num_1\n",
    "            break\n",
    "        else:\n",
    "            if num_1 == 2 :\n",
    "                matrix.loc[num,'sess_length'] = 3\n",
    "\n",
    "#print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割训练数据和测试数据\n",
    "train_data = matrix[matrix['origin'] == 'train'].drop(['origin'], axis=1)\n",
    "test_data = matrix[matrix['origin'] == 'test'].drop(['label', 'origin'], axis=1)\n",
    "train_X, train_y = train_data.drop(['label'], axis=1), train_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M= 500\n"
     ]
    }
   ],
   "source": [
    "# 使用DSIN模型\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from deepctr.inputs import SparseFeat,VarLenSparseFeat,DenseFeat,get_feature_names\n",
    "from deepctr.models import DIN, DIEN, DSIN\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "train_X['action_type']=3\n",
    "feature_columns = []\n",
    "for column in train_X.columns:\n",
    "    session_text = 'sess'\n",
    "    if session_text not in column :\n",
    "        #print(column)\n",
    "        num = train_X[column].nunique()\n",
    "        if num > 10000:\n",
    "            dim = 10\n",
    "        else:\n",
    "            if num > 1000:\n",
    "                dim = 8\n",
    "            else:\n",
    "                dim = 4\n",
    "        #print(num)\n",
    "        if column  == 'user_id':\n",
    "            feature_columns += [SparseFeat(column, 424169+1, embedding_dim=dim, use_hash=True)]\n",
    "        elif column  == 'merchant_id':\n",
    "            feature_columns += [SparseFeat(column, 4994+1, embedding_dim=dim, use_hash=True)]\n",
    "        elif column  == 'action_type':\n",
    "            feature_columns += [SparseFeat(column, 4+1, embedding_dim=dim, use_hash=True)]\n",
    "        else:\n",
    "            feature_columns += [DenseFeat(column, 1)]\n",
    "\n",
    "print('M=', M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#M = 500\n",
    "# maxlen为历史信息的长度，vocabulary_size为onehot的长度\n",
    "feature_columns += [VarLenSparseFeat(SparseFeat('sess_0_merchant_id', vocabulary_size=424169 + 1, embedding_dim=10, use_hash=True, embedding_name='merchant_id'),maxlen=M),\n",
    "                    VarLenSparseFeat(SparseFeat('sess_0_action_type', vocabulary_size=4 + 1, embedding_dim=4, use_hash=True, embedding_name='cate_id'),maxlen=M)]\n",
    "feature_columns += [VarLenSparseFeat(SparseFeat('sess_1_merchant_id', vocabulary_size=424169 + 1, embedding_dim=10, use_hash=True, embedding_name='merchant_id'),maxlen=M),\n",
    "                    VarLenSparseFeat(SparseFeat('sess_1_action_type', vocabulary_size=4 + 1, embedding_dim=4, use_hash=True, embedding_name='cate_id'),maxlen=M)]\n",
    "feature_columns += [VarLenSparseFeat(SparseFeat('sess_2_merchant_id', vocabulary_size=424169 + 1, embedding_dim=10, use_hash=True, embedding_name='merchant_id'),maxlen=M),\n",
    "                    VarLenSparseFeat(SparseFeat('sess_2_action_type', vocabulary_size=4 + 1, embedding_dim=4, use_hash=True, embedding_name='cate_id'),maxlen=M)]\n",
    "#feature_columns += [VarLenSparseFeat(SparseFeat('sess_3_merchant_id', vocabulary_size=19111 + 1, embedding_dim=8, use_hash=True, embedding_name='merchant_id'),maxlen=M),\n",
    "                    #VarLenSparseFeat(SparseFeat('sess_3_action_type', vocabulary_size=4 + 1, embedding_dim=4, use_hash=True, embedding_name='cate_id'),maxlen=M)]\n",
    "#feature_columns += [VarLenSparseFeat(SparseFeat('sess_4_merchant_id', vocabulary_size=19111 + 1, embedding_dim=8, use_hash=True, embedding_name='merchant_id'),maxlen=M),\n",
    "                    #VarLenSparseFeat(SparseFeat('sess_4_action_type', vocabulary_size=4 + 1, embedding_dim=4, use_hash=True, embedding_name='cate_id'),maxlen=M)]\n",
    "#feature_columns += [VarLenSparseFeat(SparseFeat('sess_5_merchant_id', vocabulary_size=19111 + 1, embedding_dim=8, use_hash=True, embedding_name='merchant_id'),maxlen=M),\n",
    "                    #VarLenSparseFeat(SparseFeat('sess_5_action_type', vocabulary_size=4 + 1, embedding_dim=4, use_hash=True, embedding_name='cate_id'),maxlen=M)]\n",
    "#feature_columns += [VarLenSparseFeat(SparseFeat('sess_6_merchant_id', vocabulary_size=19111 + 1, embedding_dim=8, use_hash=True, embedding_name='merchant_id'),maxlen=M),\n",
    "                    #VarLenSparseFeat(SparseFeat('sess_6_action_type', vocabulary_size=4 + 1, embedding_dim=4, use_hash=True, embedding_name='cate_id'),maxlen=M)]\n",
    "#feature_columns += [VarLenSparseFeat(SparseFeat('sess_7_merchant_id', vocabulary_size=19111 + 1, embedding_dim=8, use_hash=True, embedding_name='merchant_id'),maxlen=M),\n",
    "                    #VarLenSparseFeat(SparseFeat('sess_7_action_type', vocabulary_size=4 + 1, embedding_dim=4, use_hash=True, embedding_name='cate_id'),maxlen=M)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:117: The name tf.string_to_hash_bucket_fast is deprecated. Please use tf.strings.to_hash_bucket_fast instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\deepctr\\layers\\sequence.py:471: The name tf.keras.initializers.TruncatedNormal is deprecated. Please use tf.compat.v1.keras.initializers.TruncatedNormal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\initializers.py:94: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:117: The name tf.matrix_set_diag is deprecated. Please use tf.linalg.set_diag instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:255: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:253: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\Users\\TAOXUEJIE-PSD\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:253: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 260864/260864 [00:00<00:00, 2289720.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 260864/260864 [00:00<00:00, 1750222.62it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 260864/260864 [00:00<00:00, 1809679.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 260864/260864 [00:00<00:00, 1309024.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 260864/260864 [00:00<00:00, 1253524.84it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 260864/260864 [00:00<00:00, 659919.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 208691 samples, validate on 52173 samples\n",
      "Epoch 1/3\n",
      "208691/208691 [==============================] - 29187s 140ms/sample - loss: 0.2308 - binary_crossentropy: 0.2308 - val_loss: 0.2301 - val_binary_crossentropy: 0.2301\n",
      "Epoch 2/3\n",
      "208691/208691 [==============================] - 29088s 139ms/sample - loss: 0.2304 - binary_crossentropy: 0.2304 - val_loss: 0.2303 - val_binary_crossentropy: 0.2303\n",
      "Epoch 3/3\n",
      "208691/208691 [==============================] - 29192s 140ms/sample - loss: 0.2304 - binary_crossentropy: 0.2304 - val_loss: 0.2302 - val_binary_crossentropy: 0.2302\n"
     ]
    }
   ],
   "source": [
    "del matrix\n",
    "gc.collect()\n",
    "hist_features=['merchant_id','action_type']\n",
    "# 使用DSIN模型\n",
    "model=DSIN(feature_columns, hist_features, sess_max_count=3, att_embedding_size=3, att_head_num=4)\n",
    "# 使用Adam优化器，二分类的交叉熵\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['binary_crossentropy'])\n",
    "\n",
    "# 组装train_model_input，得到feature names，将train_X转换为字典格式\n",
    "feature_names=list(train_X.columns)\n",
    "train_model_input = {name:train_X[name].values for name in feature_names}\n",
    "# histroy输入必须是二维数组\n",
    "for fea in ['sess_0_merchant_id','sess_0_action_type',\n",
    "           'sess_1_merchant_id','sess_1_action_type',\n",
    "           'sess_2_merchant_id','sess_2_action_type']:\n",
    "           #'sess_3_merchant_id','sess_3_action_type',\n",
    "           #'sess_4_merchant_id','sess_4_action_type',\n",
    "           #'sess_5_merchant_id','sess_5_action_type',\n",
    "           #'sess_6_merchant_id','sess_6_action_type',\n",
    "           #'sess_7_merchant_id','sess_7_action_type']:\n",
    "    l = []\n",
    "    for i in tqdm(train_model_input[fea]):\n",
    "        l.append(i)\n",
    "    train_model_input[fea]=np.array(l)\n",
    "history = model.fit(train_model_input, train_y, verbose=True, epochs=3, validation_split=0.2,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 261477/261477 [00:00<00:00, 931405.40it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 261477/261477 [00:00<00:00, 494921.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 261477/261477 [00:00<00:00, 792807.80it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 261477/261477 [00:00<00:00, 1101479.03it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 261477/261477 [00:00<00:00, 907992.36it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████| 261477/261477 [00:00<00:00, 1253353.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# 转换test__model_input\n",
    "test_data['action_type']=3\n",
    "test_model_input = {name:test_data[name].values for name in feature_names}\n",
    "from tqdm import tqdm\n",
    "for fea in ['sess_0_merchant_id','sess_0_action_type',\n",
    "           'sess_1_merchant_id','sess_1_action_type',\n",
    "           'sess_2_merchant_id','sess_2_action_type']:\n",
    "    l = []\n",
    "    for i in tqdm(test_model_input[fea]):\n",
    "        l.append(i)\n",
    "    test_model_input[fea]=np.array(l)\n",
    "\n",
    "\n",
    "# 得到预测结果\n",
    "prob = model.predict(test_model_input)\n",
    "submission = pd.read_csv('./data_format1/test_format1.csv')\n",
    "submission['prob'] = prob\n",
    "submission.to_csv('prediction.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
