{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    }
   ],
   "source": [
    "#数据预处理\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dummyPy import OneHotEncoder\n",
    "import random\n",
    "import pickle  # 存储临时变量\n",
    "\n",
    "## 读文件\n",
    "#file_path = '/home/admin/avazu/'\n",
    "fp_train = 'train.csv'\n",
    "fp_test  = 'test.csv'\n",
    "\n",
    "## 下采样写文件\n",
    "fp_sub_train_f = 'sub_train_f.csv'\n",
    "fp_col_counts = 'col_counts'\n",
    "\n",
    "## data after selecting features (LR_fun needed)\n",
    "## and setting rare categories' value to 'other' (feature filtering)\n",
    "fp_train_f = 'train_f.csv'\n",
    "fp_test_f  = 'test_f.csv'\n",
    "\n",
    "## 存储标签编码和one-hot编码\n",
    "fp_lb_enc = 'lb_enc'\n",
    "fp_oh_enc = 'oh_enc'\n",
    "\n",
    "##==================== 数据预处理 ====================##\n",
    "## 特征选择\n",
    "cols = ['C1', \n",
    "        'banner_pos', \n",
    "        'site_id',\n",
    "        'site_category',\n",
    "        'app_id',\n",
    "        'app_category', \n",
    "        'device_id',\n",
    "        'device_model',\n",
    "        'device_type', \n",
    "        'device_conn_type',\n",
    "        'C14', \n",
    "        'C15',\n",
    "        'C16',\n",
    "        'C17', \n",
    "        'C18',\n",
    "        'C19',\n",
    "        'C20']\n",
    "\n",
    "cols_train = ['id', 'click']\n",
    "cols_test  = ['id']\n",
    "cols_train.extend(cols)\n",
    "cols_test.extend(cols)\n",
    "\n",
    "## 数据加载\n",
    "print('loading data...')\n",
    "df_train_ini = pd.read_csv(fp_train, nrows = 10)\n",
    "df_train_org = pd.read_csv(fp_train, chunksize = 1000000, iterator = True)\n",
    "df_test_org  = pd.read_csv(fp_test,  chunksize = 1000000, iterator = True)\n",
    "\n",
    "#----- 统计分类变量 数值个数 -----#\n",
    "## 初始化\n",
    "cols_counts = {}  # 统计每个特征的分类数量\n",
    "for col in cols:\n",
    "    cols_counts[col] = df_train_ini[col].value_counts()\n",
    "\n",
    "## 统计训练集\n",
    "for chunk in df_train_org:\n",
    "    for col in cols:\n",
    "        cols_counts[col] = cols_counts[col].append(chunk[col].value_counts())\n",
    "\n",
    "## 统计测试集\n",
    "for chunk in df_test_org:\n",
    "    for col in cols:\n",
    "        cols_counts[col] = cols_counts[col].append(chunk[col].value_counts())\n",
    "        \n",
    "## 统计\n",
    "for col in cols:\n",
    "    cols_counts[col] = cols_counts[col].groupby(cols_counts[col].index).sum()\n",
    "    # sort the counts\n",
    "    cols_counts[col] = cols_counts[col].sort_values(ascending=False)   \n",
    "\n",
    "## 存储value_counting\n",
    "pickle.dump(cols_counts, open(fp_col_counts, 'wb'))\n",
    "'''\n",
    "## 绘制分布\n",
    "fig = plt.figure(1)\n",
    "for i, col in enumerate(cols):\n",
    "    ax = fig.add_subplot(4, 3, i+1)\n",
    "    ax.fill_between(np.arange(len(cols_counts[col])), cols_counts[col].get_values())\n",
    "    # ax.set_title(col)\n",
    "plt.show()\n",
    "'''\n",
    "## 只保存前K个分类变量\n",
    "k = 99\n",
    "col_index = {}\n",
    "for col in cols:\n",
    "    col_index[col] = cols_counts[col][0: k].index\n",
    "\n",
    "df_train_org = pd.read_csv(fp_train, dtype = {'id': str}, chunksize = 1000000, iterator = True)\n",
    "df_test_org  = pd.read_csv(fp_test,  dtype = {'id': str}, chunksize = 1000000, iterator = True)\n",
    "\n",
    "## 训练集\n",
    "hd_flag = True  # add column names at 1-st row\n",
    "for chunk in df_train_org:\n",
    "    df = chunk.copy()\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype('object')\n",
    "        # assign all the rare variables as 'other'\n",
    "        df.loc[~df[col].isin(col_index[col]), col] = 'other'\n",
    "    with open(fp_train_f, 'a') as f:\n",
    "        df.to_csv(f, columns = cols_train, header = hd_flag, index = False)\n",
    "    hd_flag = False\n",
    "\n",
    "## 测试集\n",
    "hd_flag = True  # 第一个chunk需要有header\n",
    "for chunk in df_test_org:\n",
    "    df = chunk.copy()\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype('object')\n",
    "        # 设置其他不常用变量为other\n",
    "        df.loc[~df[col].isin(col_index[col]), col] = 'other'\n",
    "    with open(fp_test_f, 'a') as f:\n",
    "        df.to_csv(f, columns = cols_test, header = hd_flag, index = False)      \n",
    "    hd_flag = False    \n",
    "\n",
    "## 对分类变量进行标签编码\n",
    "lb_enc = {}\n",
    "for col in cols:\n",
    "    col_index[col] = np.append(col_index[col], 'other')\n",
    "\n",
    "for col in cols:\n",
    "    lb_enc[col] = LabelEncoder()\n",
    "    lb_enc[col].fit(col_index[col])\n",
    "    \n",
    "## 存储标签编码\n",
    "pickle.dump(lb_enc, open(fp_lb_enc, 'wb'))\n",
    "\n",
    "## one-hot编码\n",
    "oh_enc = OneHotEncoder(cols)\n",
    "\n",
    "df_train_f = pd.read_csv(fp_train_f, index_col=None, chunksize=500000, iterator=True)\n",
    "df_test_f  = pd.read_csv(fp_test_f, index_col=None, chunksize=500000, iterator=True)\n",
    "\n",
    "for chunk in df_train_f:\n",
    "    oh_enc.fit(chunk)\n",
    "for chunk in df_test_f:\n",
    "    oh_enc.fit(chunk)\n",
    "    \n",
    "## 存储one-hot编码\n",
    "pickle.dump(oh_enc, open(fp_oh_enc, 'wb'))\n",
    "\n",
    "\n",
    "# 计算总训练样本 约46M\n",
    "n = sum(1 for line in open(fp_train_f)) - 1 \n",
    "# 保存下采样训练样本 2M\n",
    "s = 2000000\n",
    "\n",
    "## 设置哪些行不需要读 skip，不需要读的行数为n-s\n",
    "skip = sorted(random.sample(range(1, n+1), n-s)) \n",
    "df_train = pd.read_csv(fp_train_f, skiprows = skip)\n",
    "df_train.columns = cols_train\n",
    "\n",
    "## 存储下采样的结果\n",
    "df_train.to_csv(fp_sub_train_f, index=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "Train on 1280000 samples, validate on 320000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280000/1280000 [==============================] - 47s 37us/sample - loss: 0.4071 - binary_crossentropy: 0.4069 - val_loss: 0.4033 - val_binary_crossentropy: 0.4030\n",
      "Epoch 2/10\n",
      "1280000/1280000 [==============================] - 40s 32us/sample - loss: 0.4030 - binary_crossentropy: 0.4026 - val_loss: 0.4013 - val_binary_crossentropy: 0.4008\n",
      "Epoch 3/10\n",
      "1280000/1280000 [==============================] - 39s 31us/sample - loss: 0.4017 - binary_crossentropy: 0.4011 - val_loss: 0.4008 - val_binary_crossentropy: 0.4002\n",
      "Epoch 4/10\n",
      "1280000/1280000 [==============================] - 39s 31us/sample - loss: 0.4008 - binary_crossentropy: 0.4002 - val_loss: 0.4012 - val_binary_crossentropy: 0.4005\n",
      "Epoch 5/10\n",
      "1280000/1280000 [==============================] - 38s 30us/sample - loss: 0.4001 - binary_crossentropy: 0.3994 - val_loss: 0.4003 - val_binary_crossentropy: 0.3996\n",
      "Epoch 6/10\n",
      "1280000/1280000 [==============================] - 37s 29us/sample - loss: 0.3997 - binary_crossentropy: 0.3990 - val_loss: 0.4000 - val_binary_crossentropy: 0.3992\n",
      "Epoch 7/10\n",
      "1280000/1280000 [==============================] - 37s 29us/sample - loss: 0.3993 - binary_crossentropy: 0.3985 - val_loss: 0.3996 - val_binary_crossentropy: 0.3988\n",
      "Epoch 8/10\n",
      "1280000/1280000 [==============================] - 37s 29us/sample - loss: 0.3990 - binary_crossentropy: 0.3981 - val_loss: 0.3995 - val_binary_crossentropy: 0.3986\n",
      "Epoch 9/10\n",
      "1280000/1280000 [==============================] - 37s 29us/sample - loss: 0.3986 - binary_crossentropy: 0.3977 - val_loss: 0.3996 - val_binary_crossentropy: 0.3987\n",
      "Epoch 10/10\n",
      "1280000/1280000 [==============================] - 37s 29us/sample - loss: 0.3984 - binary_crossentropy: 0.3975 - val_loss: 0.3999 - val_binary_crossentropy: 0.3990\n",
      "test RMSE 0.35341194094144585\n",
      "LogLoss 0.39877882473427506\n"
     ]
    }
   ],
   "source": [
    "# 使用DCN模型对Avazu CTR进行预估\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from deepctr.models import DCN #使用Deep & Cross\n",
    "from deepctr.inputs import SparseFeat,get_feature_names\n",
    "import pickle\n",
    "\n",
    "\n",
    "##==================== 设置文件路径File-Path (fp) ====================##\n",
    "#file_path = '/home/admin/avazu/'\n",
    "fp_train_f = \"sub_train_f.csv\" #使用小样本进行训练\n",
    "\n",
    "##==================== DCN 训练 ====================##\n",
    "data = pd.read_csv(fp_train_f, dtype={'id':str}, index_col=None)\n",
    "print('data loaded')\n",
    "\n",
    "#数据加载\n",
    "sparse_features = ['C1', \n",
    "        'banner_pos', \n",
    "        'site_id',\n",
    "        'site_category',\n",
    "        'app_id',\n",
    "        'app_category', \n",
    "        'device_id',\n",
    "        'device_model',\n",
    "        'device_type', \n",
    "        'device_conn_type',\n",
    "        'C14', \n",
    "        'C15',\n",
    "        'C16',\n",
    "        'C17', \n",
    "        'C18',\n",
    "        'C19',\n",
    "        'C20']\n",
    "target = ['click']\n",
    "\n",
    "# 对特征标签进行编码\n",
    "for feature in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feature] = lbe.fit_transform(data[feature])\n",
    "# 计算每个特征中的 不同特征值的个数\n",
    "fixlen_feature_columns = [SparseFeat(feature, data[feature].nunique()) for feature in sparse_features]\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "#print(fixlen_feature_columns)\n",
    "#print(feature_names)\n",
    "\n",
    "# 将数据集切分成训练集和测试集\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "train_model_input = {name:train[name].values for name in feature_names}\n",
    "test_model_input = {name:test[name].values for name in feature_names}\n",
    "\n",
    "# 使用DCN进行训练\n",
    "#model = DCN(linear_feature_columns, dnn_feature_columns, task='regression')\n",
    "model = DCN(linear_feature_columns, dnn_feature_columns, task='binary')\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "\n",
    "\n",
    "history = model.fit(train_model_input, train[target].values, batch_size=256, epochs=10, verbose=True, validation_split=0.2, )\n",
    "# 使用DCN进行预测\n",
    "pred_ans = model.predict(test_model_input, batch_size=256)\n",
    "# 输出RMSE或MSE\n",
    "mse = round(mean_squared_error(test[target].values, pred_ans), 4)\n",
    "rmse = mse ** 0.5\n",
    "print(\"test RMSE\", rmse)\n",
    "\n",
    "# 输出LogLoss\n",
    "from sklearn.metrics import log_loss\n",
    "score = log_loss(test[target].values, pred_ans)\n",
    "print(\"LogLoss\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1280000 samples, validate on 320000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280000/1280000 [==============================] - 216s 168us/sample - loss: 0.4068 - binary_crossentropy: 0.4066 - val_loss: 0.4041 - val_binary_crossentropy: 0.4038\n",
      "Epoch 2/10\n",
      "1280000/1280000 [==============================] - 215s 168us/sample - loss: 0.4027 - binary_crossentropy: 0.4023 - val_loss: 0.4016 - val_binary_crossentropy: 0.4011\n",
      "Epoch 3/10\n",
      "1280000/1280000 [==============================] - 214s 167us/sample - loss: 0.4011 - binary_crossentropy: 0.4006 - val_loss: 0.4007 - val_binary_crossentropy: 0.4002\n",
      "Epoch 4/10\n",
      "1280000/1280000 [==============================] - 217s 170us/sample - loss: 0.4003 - binary_crossentropy: 0.3997 - val_loss: 0.4003 - val_binary_crossentropy: 0.3997\n",
      "Epoch 5/10\n",
      "1280000/1280000 [==============================] - 210s 164us/sample - loss: 0.3995 - binary_crossentropy: 0.3988 - val_loss: 0.3998 - val_binary_crossentropy: 0.3991\n",
      "Epoch 6/10\n",
      "1280000/1280000 [==============================] - 215s 168us/sample - loss: 0.3990 - binary_crossentropy: 0.3982 - val_loss: 0.4000 - val_binary_crossentropy: 0.3992\n",
      "Epoch 7/10\n",
      "1280000/1280000 [==============================] - 213s 167us/sample - loss: 0.3986 - binary_crossentropy: 0.3977 - val_loss: 0.3996 - val_binary_crossentropy: 0.3988\n",
      "Epoch 8/10\n",
      "1280000/1280000 [==============================] - 212s 165us/sample - loss: 0.3981 - binary_crossentropy: 0.3973 - val_loss: 0.3998 - val_binary_crossentropy: 0.3990\n",
      "Epoch 9/10\n",
      "1280000/1280000 [==============================] - 211s 165us/sample - loss: 0.3978 - binary_crossentropy: 0.3969 - val_loss: 0.3998 - val_binary_crossentropy: 0.3989\n",
      "Epoch 10/10\n",
      "1280000/1280000 [==============================] - 211s 165us/sample - loss: 0.3974 - binary_crossentropy: 0.3965 - val_loss: 0.3995 - val_binary_crossentropy: 0.3986\n",
      "test RMSE 0.35327043465311386\n",
      "LogLoss 0.398499844920921\n"
     ]
    }
   ],
   "source": [
    "from deepctr.models import xDeepFM\n",
    "\n",
    "model = xDeepFM(linear_feature_columns, dnn_feature_columns, task='binary')\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "\n",
    "\n",
    "history = model.fit(train_model_input, train[target].values, batch_size=256, epochs=10, verbose=True, validation_split=0.2, )\n",
    "# 使用xDeepFM进行预测\n",
    "pred_ans = model.predict(test_model_input, batch_size=256)\n",
    "# 输出RMSE或MSE\n",
    "mse = round(mean_squared_error(test[target].values, pred_ans), 4)\n",
    "rmse = mse ** 0.5\n",
    "print(\"test RMSE\", rmse)\n",
    "\n",
    "# 输出LogLoss\n",
    "score = log_loss(test[target].values, pred_ans)\n",
    "print(\"LogLoss\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:06:49] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\learner.cc:328: \n",
      "Parameters: { boosting_type, subsample_freq } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[0]\ttrain-logloss:0.68824\tvalid-logloss:0.68825\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 200 rounds.\n",
      "[25]\ttrain-logloss:0.59065\tvalid-logloss:0.59164\n",
      "[50]\ttrain-logloss:0.52771\tvalid-logloss:0.52960\n",
      "[75]\ttrain-logloss:0.48555\tvalid-logloss:0.48833\n",
      "[100]\ttrain-logloss:0.45696\tvalid-logloss:0.46052\n",
      "[125]\ttrain-logloss:0.43703\tvalid-logloss:0.44145\n",
      "[150]\ttrain-logloss:0.42300\tvalid-logloss:0.42824\n",
      "[175]\ttrain-logloss:0.41302\tvalid-logloss:0.41914\n",
      "[200]\ttrain-logloss:0.40592\tvalid-logloss:0.41286\n",
      "[225]\ttrain-logloss:0.40103\tvalid-logloss:0.40860\n",
      "[250]\ttrain-logloss:0.39731\tvalid-logloss:0.40563\n",
      "[275]\ttrain-logloss:0.39454\tvalid-logloss:0.40356\n",
      "[300]\ttrain-logloss:0.39244\tvalid-logloss:0.40211\n",
      "[325]\ttrain-logloss:0.39089\tvalid-logloss:0.40113\n",
      "[350]\ttrain-logloss:0.38966\tvalid-logloss:0.40044\n",
      "[375]\ttrain-logloss:0.38859\tvalid-logloss:0.39993\n",
      "[400]\ttrain-logloss:0.38768\tvalid-logloss:0.39959\n",
      "[425]\ttrain-logloss:0.38689\tvalid-logloss:0.39933\n",
      "[450]\ttrain-logloss:0.38626\tvalid-logloss:0.39916\n",
      "[475]\ttrain-logloss:0.38568\tvalid-logloss:0.39903\n",
      "[500]\ttrain-logloss:0.38509\tvalid-logloss:0.39893\n",
      "[525]\ttrain-logloss:0.38451\tvalid-logloss:0.39887\n",
      "[550]\ttrain-logloss:0.38400\tvalid-logloss:0.39882\n",
      "[575]\ttrain-logloss:0.38348\tvalid-logloss:0.39879\n",
      "[600]\ttrain-logloss:0.38300\tvalid-logloss:0.39878\n",
      "[625]\ttrain-logloss:0.38256\tvalid-logloss:0.39878\n",
      "[650]\ttrain-logloss:0.38211\tvalid-logloss:0.39879\n",
      "[675]\ttrain-logloss:0.38174\tvalid-logloss:0.39880\n",
      "[700]\ttrain-logloss:0.38129\tvalid-logloss:0.39881\n",
      "[725]\ttrain-logloss:0.38089\tvalid-logloss:0.39884\n",
      "[750]\ttrain-logloss:0.38058\tvalid-logloss:0.39886\n",
      "[775]\ttrain-logloss:0.38027\tvalid-logloss:0.39889\n",
      "[800]\ttrain-logloss:0.37992\tvalid-logloss:0.39892\n",
      "Stopping. Best iteration:\n",
      "[618]\ttrain-logloss:0.38269\tvalid-logloss:0.39877\n",
      "\n",
      "test RMSE 0.35327043465311386\n",
      "LogLoss 0.39824099134702645\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "param = {'boosting_type':'gbdt',\n",
    "                         'objective' : 'binary:logistic', \n",
    "                         'eval_metric' : 'logloss',\n",
    "                         'eta' : 0.01,\n",
    "                         'max_depth' : 15,\n",
    "                         'colsample_bytree':0.8,\n",
    "                         'subsample': 0.9,\n",
    "                         'subsample_freq': 8,\n",
    "                         'alpha': 0.6,\n",
    "                         'lambda': 0,\n",
    "        }\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train.drop(['id','click'],axis=1), train['click'], test_size=0.2)\n",
    "\n",
    "train_data = xgb.DMatrix(X_train, label=y_train)\n",
    "valid_data = xgb.DMatrix(X_valid, label=y_valid)\n",
    "test_data = xgb.DMatrix(test.drop(['id','click'],axis=1))\n",
    "\n",
    "model_xgb = xgb.train(param, train_data, evals=[(train_data, 'train'), (valid_data, 'valid')], num_boost_round = 10000, early_stopping_rounds=200, verbose_eval=25)\n",
    "predict = model_xgb.predict(test_data)\n",
    "\n",
    "# 输出RMSE或MSE\n",
    "mse = round(mean_squared_error(test[target].values, predict), 4)\n",
    "rmse = mse ** 0.5\n",
    "print(\"test RMSE\", rmse)\n",
    "\n",
    "# 输出LogLoss\n",
    "score = log_loss(test[target].values, predict)\n",
    "print(\"LogLoss\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub = pd.read_csv('test_f.csv', dtype={'id':str})\n",
    "for feature in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    test_sub[feature] = lbe.fit_transform(test_sub[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data = xgb.DMatrix(test_sub.drop(['id'],axis=1))\n",
    "predict_xgb = model_xgb.predict(predict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sampleSubmission.csv', dtype={'id':str})\n",
    "submission['click_xgb'] = predict_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model_input = {name:test_sub[name].values for name in feature_names}\n",
    "predict_xdeepfm = model.predict(predict_model_input, batch_size=256)\n",
    "submission['click_xdeepfm'] = predict_xdeepfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['click'] = (submission['click_xdeepfm']+submission['click_xgb'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           id     click  click_xgb  click_xdeepfm\n",
      "0        10000174058809263569  0.057438   0.108411       0.006465\n",
      "1        10000182526920855428  0.024427   0.048549       0.000305\n",
      "2        10000554139829213984  0.036340   0.072475       0.000204\n",
      "3        10001094637809798845  0.009916   0.017265       0.002567\n",
      "4        10001377041558670745  0.026496   0.045275       0.007716\n",
      "5        10001521204153353724  0.143997   0.202357       0.085636\n",
      "6        10001911056707023378  0.017299   0.034508       0.000090\n",
      "7        10001982898844213216  0.006131   0.012223       0.000038\n",
      "8        10002000217531288531  0.023785   0.040772       0.006797\n",
      "9        10002107385290585663  0.024664   0.048959       0.000369\n",
      "10       10002121728305927027  0.034007   0.067972       0.000043\n",
      "11       10002246330438763746  0.024813   0.043304       0.006322\n",
      "12       10002274319697022546  0.018790   0.036070       0.001510\n",
      "13       10002614093820007721  0.013782   0.006283       0.021280\n",
      "14       10003085473872233973  0.021453   0.040384       0.002522\n",
      "15       10003460544097147762  0.012703   0.025355       0.000051\n",
      "16       10003655272218038955  0.035195   0.069597       0.000793\n",
      "17       10003886426122210904  0.032493   0.052519       0.012467\n",
      "18       10003962554013591855  0.020155   0.040081       0.000229\n",
      "19       10004028101361908315  0.038597   0.067059       0.010135\n",
      "20       10004147501365763118  0.036340   0.072475       0.000204\n",
      "21       10004218627354023598  0.039165   0.077389       0.000941\n",
      "22       10004594809490024984  0.005423   0.004982       0.005865\n",
      "23       10004595231242657912  0.065491   0.063893       0.067090\n",
      "24       10004844867110879461  0.017937   0.035799       0.000074\n",
      "25       10004882149038870218  0.024954   0.047522       0.002385\n",
      "26       10004964724620895563  0.020036   0.028427       0.011644\n",
      "27       10005270909407257114  0.015419   0.015741       0.015097\n",
      "28       10005321853961183294  0.045330   0.049521       0.041140\n",
      "29       10005377878485192027  0.090196   0.177191       0.003201\n",
      "...                       ...       ...        ...            ...\n",
      "4577434   9992570319139173288  0.017164   0.022907       0.011421\n",
      "4577435   9992804326900345174  0.022953   0.043016       0.002890\n",
      "4577436    999302493346563639  0.008894   0.017632       0.000157\n",
      "4577437   9993108024312566575  0.057376   0.092211       0.022541\n",
      "4577438   9993142888242238193  0.020269   0.030801       0.009737\n",
      "4577439   9993212674850361471  0.160035   0.216804       0.103267\n",
      "4577440   9993632655180216313  0.098922   0.136158       0.061687\n",
      "4577441   9993870862236617206  0.032051   0.062114       0.001987\n",
      "4577442   9993872887322082159  0.171854   0.181679       0.162029\n",
      "4577443   9994412868906933380  0.142201   0.149259       0.135143\n",
      "4577444   9994439259916346319  0.123289   0.114551       0.132027\n",
      "4577445    999449650781447314  0.017345   0.013964       0.020727\n",
      "4577446   9994561827501483738  0.013762   0.026410       0.001114\n",
      "4577447   9994654792261512783  0.069218   0.130557       0.007879\n",
      "4577448   9994775736488737482  0.019291   0.032893       0.005690\n",
      "4577449   9994925607916618329  0.143312   0.119258       0.167366\n",
      "4577450   9995251711418657413  0.102315   0.117218       0.087412\n",
      "4577451   9995423743902330250  0.015653   0.022047       0.009260\n",
      "4577452   9995622242746196422  0.013254   0.026188       0.000320\n",
      "4577453   9995674008643735878  0.022455   0.043757       0.001153\n",
      "4577454   9996027264563091487  0.089732   0.117468       0.061995\n",
      "4577455   9996249325501827315  0.017345   0.013964       0.020727\n",
      "4577456   9997096591855818995  0.014681   0.024899       0.004462\n",
      "4577457    999714078500708061  0.143565   0.215883       0.071247\n",
      "4577458   9997412334362334823  0.010943   0.019461       0.002425\n",
      "4577459   9998166651591969718  0.090103   0.176621       0.003585\n",
      "4577460   9998249812366450951  0.108659   0.138431       0.078886\n",
      "4577461     99988023653614546  0.091057   0.159401       0.022713\n",
      "4577462   9999086574712596585  0.009767   0.019278       0.000256\n",
      "4577463     99992636456518364  0.130691   0.137312       0.124069\n",
      "\n",
      "[4577464 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = submission.drop(['click_xdeepfm','click_xgb'],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv',index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
