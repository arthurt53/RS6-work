{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n"
     ]
    }
   ],
   "source": [
    "#数据预处理\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dummyPy import OneHotEncoder\n",
    "import random\n",
    "import pickle  # 存储临时变量\n",
    "\n",
    "## 读文件\n",
    "#file_path = '/home/admin/avazu/'\n",
    "fp_train = 'train.csv'\n",
    "fp_test  = 'test.csv'\n",
    "\n",
    "## 下采样写文件\n",
    "fp_sub_train_f = 'sub_train_f.csv'\n",
    "fp_col_counts = 'col_counts'\n",
    "\n",
    "## data after selecting features (LR_fun needed)\n",
    "## and setting rare categories' value to 'other' (feature filtering)\n",
    "fp_train_f = 'train_f.csv'\n",
    "fp_test_f  = 'test_f.csv'\n",
    "\n",
    "## 存储标签编码和one-hot编码\n",
    "fp_lb_enc = 'lb_enc'\n",
    "fp_oh_enc = 'oh_enc'\n",
    "\n",
    "##==================== 数据预处理 ====================##\n",
    "## 特征选择\n",
    "cols = ['C1', \n",
    "        'banner_pos', \n",
    "        'site_id',\n",
    "        'site_category',\n",
    "        'app_id',\n",
    "        'app_category', \n",
    "        'device_id',\n",
    "        'device_model',\n",
    "        'device_type', \n",
    "        'device_conn_type',\n",
    "        'C14', \n",
    "        'C15',\n",
    "        'C16',\n",
    "        'C17', \n",
    "        'C18',\n",
    "        'C19',\n",
    "        'C20']\n",
    "\n",
    "cols_train = ['id', 'click']\n",
    "cols_test  = ['id']\n",
    "cols_train.extend(cols)\n",
    "cols_test.extend(cols)\n",
    "\n",
    "## 数据加载\n",
    "print('loading data...')\n",
    "df_train_ini = pd.read_csv(fp_train, nrows = 10)\n",
    "df_train_org = pd.read_csv(fp_train, chunksize = 1000000, iterator = True)\n",
    "df_test_org  = pd.read_csv(fp_test,  chunksize = 1000000, iterator = True)\n",
    "\n",
    "#----- 统计分类变量 数值个数 -----#\n",
    "## 初始化\n",
    "cols_counts = {}  # 统计每个特征的分类数量\n",
    "for col in cols:\n",
    "    cols_counts[col] = df_train_ini[col].value_counts()\n",
    "\n",
    "## 统计训练集\n",
    "for chunk in df_train_org:\n",
    "    for col in cols:\n",
    "        cols_counts[col] = cols_counts[col].append(chunk[col].value_counts())\n",
    "\n",
    "## 统计测试集\n",
    "for chunk in df_test_org:\n",
    "    for col in cols:\n",
    "        cols_counts[col] = cols_counts[col].append(chunk[col].value_counts())\n",
    "        \n",
    "## 统计\n",
    "for col in cols:\n",
    "    cols_counts[col] = cols_counts[col].groupby(cols_counts[col].index).sum()\n",
    "    # sort the counts\n",
    "    cols_counts[col] = cols_counts[col].sort_values(ascending=False)   \n",
    "\n",
    "## 存储value_counting\n",
    "pickle.dump(cols_counts, open(fp_col_counts, 'wb'))\n",
    "'''\n",
    "## 绘制分布\n",
    "fig = plt.figure(1)\n",
    "for i, col in enumerate(cols):\n",
    "    ax = fig.add_subplot(4, 3, i+1)\n",
    "    ax.fill_between(np.arange(len(cols_counts[col])), cols_counts[col].get_values())\n",
    "    # ax.set_title(col)\n",
    "plt.show()\n",
    "'''\n",
    "## 只保存前K个分类变量\n",
    "k = 99\n",
    "col_index = {}\n",
    "for col in cols:\n",
    "    col_index[col] = cols_counts[col][0: k].index\n",
    "\n",
    "df_train_org = pd.read_csv(fp_train, dtype = {'id': str}, chunksize = 1000000, iterator = True)\n",
    "df_test_org  = pd.read_csv(fp_test,  dtype = {'id': str}, chunksize = 1000000, iterator = True)\n",
    "\n",
    "## 训练集\n",
    "hd_flag = True  # add column names at 1-st row\n",
    "for chunk in df_train_org:\n",
    "    df = chunk.copy()\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype('object')\n",
    "        # assign all the rare variables as 'other'\n",
    "        df.loc[~df[col].isin(col_index[col]), col] = 'other'\n",
    "    with open(fp_train_f, 'a') as f:\n",
    "        df.to_csv(f, columns = cols_train, header = hd_flag, index = False)\n",
    "    hd_flag = False\n",
    "\n",
    "## 测试集\n",
    "hd_flag = True  # 第一个chunk需要有header\n",
    "for chunk in df_test_org:\n",
    "    df = chunk.copy()\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype('object')\n",
    "        # 设置其他不常用变量为other\n",
    "        df.loc[~df[col].isin(col_index[col]), col] = 'other'\n",
    "    with open(fp_test_f, 'a') as f:\n",
    "        df.to_csv(f, columns = cols_test, header = hd_flag, index = False)      \n",
    "    hd_flag = False    \n",
    "\n",
    "## 对分类变量进行标签编码\n",
    "lb_enc = {}\n",
    "for col in cols:\n",
    "    col_index[col] = np.append(col_index[col], 'other')\n",
    "\n",
    "for col in cols:\n",
    "    lb_enc[col] = LabelEncoder()\n",
    "    lb_enc[col].fit(col_index[col])\n",
    "    \n",
    "## 存储标签编码\n",
    "pickle.dump(lb_enc, open(fp_lb_enc, 'wb'))\n",
    "\n",
    "## one-hot编码\n",
    "oh_enc = OneHotEncoder(cols)\n",
    "\n",
    "df_train_f = pd.read_csv(fp_train_f, index_col=None, chunksize=500000, iterator=True)\n",
    "df_test_f  = pd.read_csv(fp_test_f, index_col=None, chunksize=500000, iterator=True)\n",
    "\n",
    "for chunk in df_train_f:\n",
    "    oh_enc.fit(chunk)\n",
    "for chunk in df_test_f:\n",
    "    oh_enc.fit(chunk)\n",
    "    \n",
    "## 存储one-hot编码\n",
    "pickle.dump(oh_enc, open(fp_oh_enc, 'wb'))\n",
    "\n",
    "\n",
    "# 计算总训练样本 约46M\n",
    "n = sum(1 for line in open(fp_train_f)) - 1 \n",
    "# 保存下采样训练样本 2M\n",
    "s = 10000000\n",
    "\n",
    "## 设置哪些行不需要读 skip，不需要读的行数为n-s\n",
    "skip = sorted(random.sample(range(1, n+1), n-s)) \n",
    "df_train = pd.read_csv(fp_train_f, skiprows = skip)\n",
    "df_train.columns = cols_train\n",
    "\n",
    "## 存储下采样的结果\n",
    "df_train.to_csv(fp_sub_train_f, index=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "Train on 6400000 samples, validate on 1600000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400000/6400000 [==============================] - 1083s 169us/sample - loss: 0.4032 - binary_crossentropy: 0.4027 - val_loss: 0.4000 - val_binary_crossentropy: 0.3993\n",
      "Epoch 2/10\n",
      "6400000/6400000 [==============================] - 1091s 170us/sample - loss: 0.4003 - binary_crossentropy: 0.3996 - val_loss: 0.3992 - val_binary_crossentropy: 0.3984\n",
      "Epoch 3/10\n",
      "6400000/6400000 [==============================] - 1081s 169us/sample - loss: 0.3996 - binary_crossentropy: 0.3987 - val_loss: 0.3983 - val_binary_crossentropy: 0.3975\n",
      "Epoch 4/10\n",
      "6400000/6400000 [==============================] - 1080s 169us/sample - loss: 0.3991 - binary_crossentropy: 0.3983 - val_loss: 0.3985 - val_binary_crossentropy: 0.3976\n",
      "Epoch 5/10\n",
      "6400000/6400000 [==============================] - 1071s 167us/sample - loss: 0.3989 - binary_crossentropy: 0.3980 - val_loss: 0.3982 - val_binary_crossentropy: 0.3973\n",
      "Epoch 6/10\n",
      "6400000/6400000 [==============================] - 1065s 166us/sample - loss: 0.3986 - binary_crossentropy: 0.3978 - val_loss: 0.3985 - val_binary_crossentropy: 0.3976\n",
      "Epoch 7/10\n",
      "6400000/6400000 [==============================] - 1083s 169us/sample - loss: 0.3985 - binary_crossentropy: 0.3976 - val_loss: 0.3979 - val_binary_crossentropy: 0.3970\n",
      "Epoch 8/10\n",
      "6400000/6400000 [==============================] - 1071s 167us/sample - loss: 0.3983 - binary_crossentropy: 0.3974 - val_loss: 0.3980 - val_binary_crossentropy: 0.3970\n",
      "Epoch 9/10\n",
      "6400000/6400000 [==============================] - 1077s 168us/sample - loss: 0.3982 - binary_crossentropy: 0.3973 - val_loss: 0.3978 - val_binary_crossentropy: 0.3969 - loss: 0.3982 - binary_crossentro\n",
      "Epoch 10/10\n",
      "6400000/6400000 [==============================] - 1081s 169us/sample - loss: 0.3981 - binary_crossentropy: 0.3972 - val_loss: 0.3980 - val_binary_crossentropy: 0.3971\n",
      "test RMSE 0.3535533905932738\n",
      "LogLoss 0.39876091923294543\n"
     ]
    }
   ],
   "source": [
    "# 使用xDeepFM 模型对Avazu CTR进行预估\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from deepctr.models import xDeepFM\n",
    "from deepctr.inputs import SparseFeat,get_feature_names\n",
    "import pickle\n",
    "\n",
    "\n",
    "##==================== 设置文件路径File-Path (fp) ====================##\n",
    "#file_path = '/home/admin/avazu/'\n",
    "fp_train_f = \"sub_train_f.csv\" #使用小样本进行训练\n",
    "\n",
    "##==================== xDeepFM 训练 ====================##\n",
    "data = pd.read_csv(fp_train_f, dtype={'id':str}, index_col=None)\n",
    "print('data loaded')\n",
    "\n",
    "#数据加载\n",
    "sparse_features = ['C1', \n",
    "        'banner_pos', \n",
    "        'site_id',\n",
    "        'site_category',\n",
    "        'app_id',\n",
    "        'app_category', \n",
    "        'device_id',\n",
    "        'device_model',\n",
    "        'device_type', \n",
    "        'device_conn_type',\n",
    "        'C14', \n",
    "        'C15',\n",
    "        'C16',\n",
    "        'C17', \n",
    "        'C18',\n",
    "        'C19',\n",
    "        'C20']\n",
    "target = ['click']\n",
    "\n",
    "# 对特征标签进行编码\n",
    "for feature in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    data[feature] = lbe.fit_transform(data[feature])\n",
    "# 计算每个特征中的 不同特征值的个数\n",
    "fixlen_feature_columns = [SparseFeat(feature, data[feature].nunique()) for feature in sparse_features]\n",
    "linear_feature_columns = fixlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "#print(fixlen_feature_columns)\n",
    "#print(feature_names)\n",
    "\n",
    "# 将数据集切分成训练集和测试集\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "train_model_input = {name:train[name].values for name in feature_names}\n",
    "test_model_input = {name:test[name].values for name in feature_names}\n",
    "\n",
    "\n",
    "model = xDeepFM(linear_feature_columns, dnn_feature_columns, task='binary')\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'], )\n",
    "\n",
    "\n",
    "history = model.fit(train_model_input, train[target].values, batch_size=256, epochs=10, verbose=True, validation_split=0.2, )\n",
    "# 使用xDeepFM进行预测\n",
    "pred_ans = model.predict(test_model_input, batch_size=256)\n",
    "# 输出RMSE或MSE\n",
    "mse = round(mean_squared_error(test[target].values, pred_ans), 4)\n",
    "rmse = mse ** 0.5\n",
    "print(\"test RMSE\", rmse)\n",
    "\n",
    "# 输出LogLoss\n",
    "score = log_loss(test[target].values, pred_ans)\n",
    "print(\"LogLoss\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:11:41] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\learner.cc:328: \n",
      "Parameters: { boosting_type, subsample_freq } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:11:44] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.0.0\\src\\gbm\\gbtree.cc:138: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[0]\ttrain-logloss:0.68875\tvalid-logloss:0.68846\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 200 rounds.\n",
      "[25]\ttrain-logloss:0.59132\tvalid-logloss:0.59149\n",
      "[50]\ttrain-logloss:0.52880\tvalid-logloss:0.52930\n",
      "[75]\ttrain-logloss:0.48730\tvalid-logloss:0.48796\n",
      "[100]\ttrain-logloss:0.45914\tvalid-logloss:0.46009\n",
      "[125]\ttrain-logloss:0.43989\tvalid-logloss:0.44099\n",
      "[150]\ttrain-logloss:0.42642\tvalid-logloss:0.42784\n",
      "[175]\ttrain-logloss:0.41707\tvalid-logloss:0.41874\n",
      "[200]\ttrain-logloss:0.41048\tvalid-logloss:0.41242\n",
      "[225]\ttrain-logloss:0.40602\tvalid-logloss:0.40812\n",
      "[250]\ttrain-logloss:0.40285\tvalid-logloss:0.40507\n",
      "[275]\ttrain-logloss:0.40049\tvalid-logloss:0.40293\n",
      "[300]\ttrain-logloss:0.39886\tvalid-logloss:0.40144\n",
      "[325]\ttrain-logloss:0.39750\tvalid-logloss:0.40034\n",
      "[350]\ttrain-logloss:0.39660\tvalid-logloss:0.39957\n",
      "[375]\ttrain-logloss:0.39587\tvalid-logloss:0.39897\n",
      "[400]\ttrain-logloss:0.39530\tvalid-logloss:0.39857\n",
      "[425]\ttrain-logloss:0.39482\tvalid-logloss:0.39827\n",
      "[450]\ttrain-logloss:0.39438\tvalid-logloss:0.39801\n",
      "[475]\ttrain-logloss:0.39399\tvalid-logloss:0.39777\n",
      "[500]\ttrain-logloss:0.39364\tvalid-logloss:0.39760\n",
      "[525]\ttrain-logloss:0.39331\tvalid-logloss:0.39745\n",
      "[550]\ttrain-logloss:0.39301\tvalid-logloss:0.39734\n",
      "[575]\ttrain-logloss:0.39269\tvalid-logloss:0.39722\n",
      "[600]\ttrain-logloss:0.39246\tvalid-logloss:0.39712\n",
      "[625]\ttrain-logloss:0.39222\tvalid-logloss:0.39703\n",
      "[650]\ttrain-logloss:0.39194\tvalid-logloss:0.39693\n",
      "[675]\ttrain-logloss:0.39171\tvalid-logloss:0.39686\n",
      "[700]\ttrain-logloss:0.39147\tvalid-logloss:0.39678\n",
      "[725]\ttrain-logloss:0.39125\tvalid-logloss:0.39673\n",
      "[750]\ttrain-logloss:0.39105\tvalid-logloss:0.39668\n",
      "[775]\ttrain-logloss:0.39089\tvalid-logloss:0.39664\n",
      "[800]\ttrain-logloss:0.39070\tvalid-logloss:0.39660\n",
      "[825]\ttrain-logloss:0.39051\tvalid-logloss:0.39657\n",
      "[850]\ttrain-logloss:0.39035\tvalid-logloss:0.39654\n",
      "[875]\ttrain-logloss:0.39017\tvalid-logloss:0.39652\n",
      "[900]\ttrain-logloss:0.39005\tvalid-logloss:0.39650\n",
      "[925]\ttrain-logloss:0.38991\tvalid-logloss:0.39648\n",
      "[950]\ttrain-logloss:0.38977\tvalid-logloss:0.39646\n",
      "[975]\ttrain-logloss:0.38963\tvalid-logloss:0.39645\n",
      "[1000]\ttrain-logloss:0.38950\tvalid-logloss:0.39644\n",
      "[1025]\ttrain-logloss:0.38934\tvalid-logloss:0.39643\n",
      "[1050]\ttrain-logloss:0.38921\tvalid-logloss:0.39643\n",
      "[1075]\ttrain-logloss:0.38909\tvalid-logloss:0.39642\n",
      "[1100]\ttrain-logloss:0.38898\tvalid-logloss:0.39642\n",
      "[1125]\ttrain-logloss:0.38886\tvalid-logloss:0.39642\n",
      "[1150]\ttrain-logloss:0.38875\tvalid-logloss:0.39642\n",
      "[1175]\ttrain-logloss:0.38862\tvalid-logloss:0.39643\n",
      "[1200]\ttrain-logloss:0.38850\tvalid-logloss:0.39643\n",
      "[1225]\ttrain-logloss:0.38839\tvalid-logloss:0.39644\n",
      "[1250]\ttrain-logloss:0.38830\tvalid-logloss:0.39644\n",
      "[1275]\ttrain-logloss:0.38819\tvalid-logloss:0.39645\n",
      "Stopping. Best iteration:\n",
      "[1094]\ttrain-logloss:0.38900\tvalid-logloss:0.39642\n",
      "\n",
      "test RMSE 0.3531288716601915\n",
      "LogLoss 0.3973288369509726\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "param = {'boosting_type':'gbdt',\n",
    "                         'objective' : 'binary:logistic', \n",
    "                         'eval_metric' : 'logloss',\n",
    "                         'eta' : 0.01,\n",
    "                         'max_depth' : 15,\n",
    "                         'colsample_bytree':0.8,\n",
    "                         'subsample': 0.9,\n",
    "                         'subsample_freq': 8,\n",
    "                         'alpha': 0.6,\n",
    "                         'lambda': 0,\n",
    "        }\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train.drop(['id','click'],axis=1), train['click'], test_size=0.2)\n",
    "\n",
    "train_data = xgb.DMatrix(X_train, label=y_train)\n",
    "valid_data = xgb.DMatrix(X_valid, label=y_valid)\n",
    "test_data = xgb.DMatrix(test.drop(['id','click'],axis=1))\n",
    "\n",
    "model_xgb = xgb.train(param, train_data, evals=[(train_data, 'train'), (valid_data, 'valid')], num_boost_round = 10000, early_stopping_rounds=200, verbose_eval=25)\n",
    "predict = model_xgb.predict(test_data)\n",
    "\n",
    "# 输出RMSE或MSE\n",
    "mse = round(mean_squared_error(test[target].values, predict), 4)\n",
    "rmse = mse ** 0.5\n",
    "print(\"test RMSE\", rmse)\n",
    "\n",
    "# 输出LogLoss\n",
    "score = log_loss(test[target].values, predict)\n",
    "print(\"LogLoss\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub = pd.read_csv('test_f.csv', dtype={'id':str})\n",
    "for feature in sparse_features:\n",
    "    lbe = LabelEncoder()\n",
    "    test_sub[feature] = lbe.fit_transform(test_sub[feature])\n",
    "\n",
    "predict_data = xgb.DMatrix(test_sub.drop(['id'],axis=1))\n",
    "predict_xgb = model_xgb.predict(predict_data)\n",
    "\n",
    "submission = pd.read_csv('sampleSubmission.csv', dtype={'id':str})\n",
    "submission['click_xgb'] = predict_xgb\n",
    "\n",
    "predict_model_input = {name:test_sub[name].values for name in feature_names}\n",
    "predict_xdeepfm = model.predict(predict_model_input, batch_size=256)\n",
    "submission['click_xdeepfm'] = predict_xdeepfm\n",
    "\n",
    "submission['click'] = (submission['click_xdeepfm']+submission['click_xgb'])/2\n",
    "\n",
    "submission = submission.drop(['click_xdeepfm','click_xgb'],axis =1)\n",
    "\n",
    "submission.to_csv('submission.csv',index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
